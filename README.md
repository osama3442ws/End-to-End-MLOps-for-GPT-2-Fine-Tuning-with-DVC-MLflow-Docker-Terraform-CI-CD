# âœ¨ Fine-Tune GPT-2 with DVC, MLflow, Docker, Terraform & CI/CD

## ğŸ“œ Project Overview

Fine-tune GPT-2 on a creative-writing dataset, track experiments, and version data/models with production-friendly tooling:

* ğŸ“¦ **DVC** for data/model versioning & reproducible pipelines
* ğŸ“Š **MLflow** for experiment tracking
* â˜ï¸ **AWS S3** for remote storage of large artifacts
* ğŸ³ **Docker & Docker Compose** for consistent runtime
* ğŸ—ï¸ **Terraform** to provision AWS infrastructure
* ğŸ¤– **GitHub Actions** for CI/CD automation

**Result:**
ğŸ“‚ `fine_tuned_gpt2/` â†’ fine-tuned model
ğŸ“„ `metrics.json` â†’ DVC metrics
ğŸ“ `artifacts/samples/` â†’ sample generations

---

## ğŸ—ï¸ Architecture (High Level)

* ğŸ“‚ Code & pipeline definitions â†’ **Git** (this repo)
* âš™ï¸ **DVC** orchestrates the pipeline & stores big files in **S3**
* ğŸ“Š **MLflow** logs metrics/params for every training run
* ğŸ³ **Docker** ensures identical environments locally & in CI
* ğŸ—ï¸ **Terraform** provisions S3, ECR, EC2, IAM roles, and security groups
* ğŸ”„ **GitHub Actions** runs `dvc repro` on push â†’ pushes artifacts to S3

ğŸ“Œ Diagram: `Screenshot 2025-07-23 090851.png`

---

## ğŸ’» Tech Stack

| Purpose           | Tools                         |
| ----------------- | ----------------------------- |
| **Model**         | ğŸ¤— `transformers` (GPT-2)     |
| **Data**          | ğŸ—‚ `datasets`, `pandas`       |
| **Tracking**      | ğŸ“Š `mlflow`                   |
| **Orchestration** | âš™ï¸ `dvc`                      |
| **Runtime**       | ğŸ³ `docker`, `docker-compose` |
| **Storage**       | â˜ï¸ AWS S3                     |
| **Infra**         | ğŸ—ï¸ Terraform                 |
| **CI/CD**         | ğŸ¤– GitHub Actions             |

---

## ğŸ“‚ Repository Structure

```text
.
â”œâ”€â”€ config.py                 # Loads config, overrides from params.yaml
â”œâ”€â”€ params.yaml               # Single source of hyperparameters and paths
â”œâ”€â”€ dvc.yaml                  # Pipeline definition
â”œâ”€â”€ data_processing.py        # Data cleaning, tokenization, split
â”œâ”€â”€ model.py                  # Training + MLflow logging
â”œâ”€â”€ inference.py              # CLI text generation
â”œâ”€â”€ main.py                   # Entry point
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ Dockerfile                # Container setup
â”œâ”€â”€ docker-compose.yml        # Services for training/MLflow/Gradio
â”œâ”€â”€ creative_writing_dataset.csv
â”œâ”€â”€ cleaned_creative_writing_dataset.csv
â”œâ”€â”€ terraform/                # Infrastructure as code
â””â”€â”€ .github/workflows/dvc-repro.yml  # CI workflow
```

---

## ğŸ”„ How the Pipeline Works

**Stages in `dvc.yaml`:**

1. **ğŸ§¹ preprocess**

   * Cleans dataset â†’ `cleaned_creative_writing_dataset.csv` (DVC-tracked)

2. **ğŸ”¤ tokenize**

   * Tokenizes with GPT-2 tokenizer â†’ `artifacts/processed/` (DVC-tracked)

3. **ğŸ§  train**

   * Fine-tunes GPT-2, logs to MLflow, creates `metrics.json`

4. **âœï¸ generate**

   * Generates sample text â†’ `artifacts/samples/sample.txt`

ğŸ’¡ **Why DVC?** Guarantees reproducibility & versions large files.
ğŸ’¡ **Why MLflow?** Logs hyperparams & metrics for comparison.

---

## âš™ï¸ Configuration

**`params.yaml`** = single source of truth:

* ğŸ“‚ Data: file paths & column names
* ğŸ§  Model: name, output directory
* ğŸ¯ Training: learning rate, batch size, epochs, weight decay, early stopping, max length, test size
* âœï¸ Inference: generation length, temperature, top\_k, prompt

**`config.py`** loads defaults & overrides from `params.yaml`.

---

## ğŸš€ Getting Started (Local)

**Prerequisites:**

* Python 3.10+
* Git
* AWS CLI (optional)
* S3 bucket (optional)

```powershell
python -m venv .venv; .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt

# DVC init
dvc init

# Optional: configure S3
$env:AWS_ACCESS_KEY_ID="..."
$env:AWS_SECRET_ACCESS_KEY="..."
$env:AWS_DEFAULT_REGION="eu-west-1"
dvc remote add -d s3remote s3://<your-bucket>/dvcstore
dvc remote modify s3remote region eu-west-1
```

Run pipeline:

```powershell
dvc repro
dvc push
```

---

## ğŸ³ Docker & Docker Compose

```powershell
docker build -t creative-gpt2 .
$env:ECR_REPOSITORY="creative-gpt2"; $env:IMAGE_TAG="local"
docker compose up -d
```

* **train** â†’ runs `dvc pull â†’ train â†’ dvc push`
* **gradio** â†’ serves model on `http://localhost:7860`
* **mlflow** â†’ UI on `http://localhost:5000`

---

## ğŸ—ï¸ Infrastructure with Terraform

### ğŸ“¦ What Terraform Provisions

* â˜ï¸ **S3 Bucket** (`<project_name>-dvc-bucket`) â€” Remote storage for DVC artifacts
* ğŸ“¦ **ECR Repository** â€” Stores Docker images for deployment
* ğŸ” **Security Group** â€” Opens 22 (SSH), 7860 (Gradio), 5000 (MLflow)
* ğŸ–¥ **EC2 Instance (Ubuntu)** â€” Runs Docker & Docker Compose
* ğŸ‘¤ **IAM Role** â€” Grants EC2 access to S3 and ECR without hardcoding keys
* ğŸ“¤ **Outputs**:

  * `dvc_bucket_name` â€” DVC remote
  * `ecr_repository_url` â€” Docker image tag
  * `ec2_public_ip` â€” SSH access

### ğŸ”— How It Fits the Pipeline

* **DVC remote** â†’ S3 bucket created by Terraform
* **Docker image** â†’ Built locally/CI and pushed to ECR
* **EC2 runtime** â†’ Pulls image and runs pipeline via Docker Compose
* **Ports** â†’ Open for accessing Gradio & MLflow from your machine

### ğŸš€ Deploy with Terraform

```bash
cd terraform
terraform init
terraform plan -var "aws_region=eu-west-1" -var "project_name=llmops-gpt2" -var "key_name=<your-keypair>"
terraform apply -auto-approve -var "aws_region=eu-west-1" -var "project_name=llmops-gpt2" -var "key_name=<your-keypair>"
```

Copy the outputs:

* `dvc_bucket_name` â†’ `s3://<dvc_bucket_name>/dvcstore`
* `ecr_repository_url` â†’ for tagging/pushing image
* `ec2_public_ip` â†’ `ssh ubuntu@<ec2_public_ip>`

### ğŸ³ Build & Push Image to ECR

```bash
aws ecr get-login-password --region <region> | docker login --username AWS --password-stdin <account>.dkr.ecr.<region>.amazonaws.com
docker build -t creative-gpt2 .
docker tag creative-gpt2:latest <ecr_repository_url>:latest
docker push <ecr_repository_url>:latest
```

### â–¶ï¸ Run on EC2

```bash
ssh ubuntu@<ec2_public_ip>
sudo apt-get update && sudo apt-get install -y git
git clone <your-repo.git> app && cd app
export ECR_REPOSITORY="<ecr_repository_url>"
export IMAGE_TAG="latest"
docker compose up -d
```

### ğŸ”’ Security Notes

* Security group currently allows `0.0.0.0/0` on 22/7860/5000 â€” restrict for production
* `force_destroy = true` on S3 eases cleanup but deletes all objects â€” remove for safety

---

## ğŸ¤– CI/CD with GitHub Actions

* Runs on every push to `main`
* Installs deps + `dvc[s3]`
* Configures AWS from secrets
* `dvc pull â†’ dvc repro â†’ dvc push`
* Uploads `metrics.json` + samples

**Required Secrets:**

* `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`
* `DVC_REMOTE_URL` = `s3://<dvc_bucket_name>/dvcstore`

---

## ğŸ“ˆ Typical End-to-End Flow

1. Edit `params.yaml`
2. Commit & push
3. CI runs pipeline, uploads artifacts to S3
4. Gradio pulls newest model
5. Check MLflow/metrics.json for results

---

## ğŸ›  Tips & Troubleshooting

* Donâ€™t commit large files â€” let DVC track them
* Verify AWS credentials if S3 auth fails
* Ensure internet access for Hugging Face downloads
* Quick experiments:

  ```powershell
  dvc exp run -S training.num_train_epochs=5
  ```

---

ğŸ“Œ Diagram available at: `Screenshot 2025-07-23 090851.png`

---

